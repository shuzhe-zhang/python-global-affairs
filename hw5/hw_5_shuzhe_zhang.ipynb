{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7fd622f-35f3-4c1b-bcda-75858b33916d",
   "metadata": {},
   "source": [
    "1. In class we wrote code to get all of the links from the G77 statements website: https://www.g77.org/statement/index.php\n",
    "Please write code that iterates through each of the links and formats them properly so that they are a readable url. Print them to the console.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d104353-eec8-448a-bb24-fd8ac0917544",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.g77.org/statement/#jan\n",
      "https://www.g77.org/statement/#jul\n",
      "https://www.g77.org/statement/#feb\n",
      "https://www.g77.org/statement/#aug\n",
      "https://www.g77.org/statement/#mar\n",
      "https://www.g77.org/statement/#sep\n",
      "https://www.g77.org/statement/#apr\n",
      "https://www.g77.org/statement/#oct\n",
      "https://www.g77.org/statement/#may\n",
      "https://www.g77.org/statement/#nov\n",
      "https://www.g77.org/statement/#jun\n",
      "https://www.g77.org/statement/#dec\n",
      "https://www.g77.org/statement/getstatement.php?id=240215\n",
      "https://www.g77.org/statement/getstatement.php?id=240208\n",
      "https://www.g77.org/statement/getstatement.php?id=240207b\n",
      "https://www.g77.org/statement/getstatement.php?id=240207\n",
      "https://www.g77.org/statement/getstatement.php?id=240206b\n",
      "https://www.g77.org/statement/getstatement.php?id=240206\n",
      "https://www.g77.org/statement/getstatement.php?id=240205c\n",
      "https://www.g77.org/statement/getstatement.php?id=240205b\n",
      "https://www.g77.org/statement/getstatement.php?id=240205\n",
      "https://www.g77.org/statement/getstatement.php?id=240202\n",
      "https://www.g77.org/statement/#\n",
      "https://www.g77.org/statement/getstatement.php?id=240122\n",
      "https://www.g77.org/statement/getstatement.php?id=240121b\n",
      "https://www.g77.org/statement/getstatement.php?id=240121\n",
      "https://www.g77.org/statement/getstatement.php?id=240116\n",
      "https://www.g77.org/statement/#\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.g77.org/statement\"\n",
    "response = requests.get(url)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "    links = soup.find_all(\"a\")\n",
    "    # Create an empty list for all the URLs\n",
    "    url_list = []\n",
    "    # Extract and print the href attributes from the links\n",
    "    for link in links:\n",
    "        href = link.get(\"href\")\n",
    "        if href and href != \"javascript:;\": # Ignore JavaScript links\n",
    "            if href[0] == \"#\":\n",
    "                url_str = \"https://www.g77.org/statement/\" + href\n",
    "            else:\n",
    "                url_str = \"https://www.g77.org\" + href\n",
    "            \n",
    "            print(url_str)\n",
    "            url_list.append(url_str)\n",
    "else:\n",
    "    print(f\"Failed to retrieve content: HTTP {response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7459f03-1ae9-4042-9a2d-b3cf8e419e39",
   "metadata": {},
   "source": [
    "2. Now modify your code so that the urls are saved as a list. Iterate through the list, open each url, and extract the text from each, and save the text to a file called G77_2024."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "043a0964-f071-4ac1-af9d-623f32fd1d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No meaningful content found in https://www.g77.org/statement/#jan\n",
      "No meaningful content found in https://www.g77.org/statement/#jul\n",
      "No meaningful content found in https://www.g77.org/statement/#feb\n",
      "No meaningful content found in https://www.g77.org/statement/#aug\n",
      "No meaningful content found in https://www.g77.org/statement/#mar\n",
      "No meaningful content found in https://www.g77.org/statement/#sep\n",
      "No meaningful content found in https://www.g77.org/statement/#apr\n",
      "No meaningful content found in https://www.g77.org/statement/#oct\n",
      "No meaningful content found in https://www.g77.org/statement/#may\n",
      "No meaningful content found in https://www.g77.org/statement/#nov\n",
      "No meaningful content found in https://www.g77.org/statement/#jun\n",
      "No meaningful content found in https://www.g77.org/statement/#dec\n",
      "No meaningful content found in https://www.g77.org/statement/#\n",
      "No meaningful content found in https://www.g77.org/statement/#\n"
     ]
    }
   ],
   "source": [
    "with open(\"G77_2024.txt\", \"w\") as file:\n",
    "    for url in url_list:\n",
    "        response = requests.get(url)\n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "            paragraphs = soup.find_all(\"p\")\n",
    "            statement_text = \"\\n\".join([paragraph.text.strip() for paragraph in paragraphs])\n",
    "            # Write to file only if there's meaningful content\n",
    "            if statement_text:\n",
    "                file.write(statement_text + \"\\n\\n---\\n\\n\")\n",
    "            else:\n",
    "                print(f\"No meaningful content found in {url}\")\n",
    "        else:\n",
    "            print(f\"Failed to retrieve content from {url}: HTTP {resp.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dffbe2f5-8be0-4471-909d-6eb16c99df5b",
   "metadata": {},
   "source": [
    "3. Perform a word frequency distribution on the text and visualize the top 20 words used in these statements.( Please remove any stopwords first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6273778c-2dfd-441d-a816-ae9fc617357e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8757724c-cabc-434a-bfcb-0cadf572c9cb",
   "metadata": {},
   "source": [
    "4. Here is a link to a speech made by President Trump on January 6: https://www.npr.org/2021/02/10/966396848/read-trumps-jan-6-speech-a-key-part-of-impeachment-trial. Not all of the text is his speech. Some of the text is the analysis. Please extract Donald Trumps speech only, remove stopwords and perform a word frequency distribution and visualize it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0840e610-5927-4e83-8f14-61ac6090eb5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbf40bd-b008-428d-81cb-33e73a0d79b3",
   "metadata": {},
   "source": [
    "5. Here is a link to a github repo that contains Donald Trump's speeches: https://github.com/ryanmcdermott/trump-speeches/blob/master/speeches.txt\n",
    "\n",
    "What are the 10 most common things Donald Trump \"loves?\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacf2c16-db85-476f-a5a4-f06b6c8c71a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7254701c-1520-4485-a360-63c955396e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "6. Which are the top 5 countries Trump mentions in his speeches, besides America or the United States of America?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8e2897-7aee-4e1d-9830-e19603953c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6a6463-1686-44f0-b155-caa1537090ac",
   "metadata": {},
   "source": [
    "In class we talked about sentence tokenizers that could be useful in determining when sentences might be repeated. One application we discussed was in political speeches, when certain sentences or phrases are repeated to show a candidates coaching, focus or in marketing terms, \"messaging.\" Here is an analyis from https://www.vox.com/2016/8/18/12423688/donald-trump-speech-style-explained-by-linguists of the \"salesman\" techniques that Trump uses:\n",
    "\"Trump’s speeches can be appealing because he uses a lot of salesmen’s tricks. Lakoff, for his part, has an explanation for why Trump’s style of speaking is so appealing to many. Many of Trump’s most famous catchphrases are actually versions of time-tested speech mechanisms that salesmen use. They’re powerful because they help shape our unconscious. Take, for example, Trump’s frequent use of \"Many people are saying...\" or \"Believe me\" — often right after saying something that is baseless or untrue. This tends to sound more trustworthy to listeners than just outright stating the baseless claim, since Trump implies that he has direct experience with what he’s talking about. At a base level, Lakoff argues, people are more inclined to believe something that seems to have been shared. Or when Trump keeps calling Clinton \"crooked,\" or keeps referring to terrorists as \"radical Muslims,\" he’s strengthening the association through repetition. He also calls his supporters \"folks,\" to show he is one of them (though many politicians employ this trick). Trump doesn’t repeat phrases and adjectives because he is stalling for time, Liberman says; for the most part, he’s providing emphasis and strengthening the association.\n",
    "These are normal techniques, particularly in conversational speech. \"Is he reading cognitive science? No. He has 50 years of experience as a salesman who doesn’t care who he is selling to,\" Lakoff says. On this account, Trump uses similar methods in his QVC-style pitch of steaks and vodka as when he talks about his plan to stop ISIS.\"He has been doing this for a very long time as a salesman — that’s what he is best at,\" Lakoff says.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367b87fc-e344-4fdf-846b-4b3b52eeb745",
   "metadata": {},
   "source": [
    "\n",
    "7. Perform a frequency analysis that provides evidence for the assertion made in the Vox article. \n",
    "\n",
    "Consider tokenizing by unigram (one word), bigram(two words), trigram(three words) or more, or whole sentences or multiple approaches that help us understand the most common Trump linguistic characteristcs. Use your evidence and words to describe what you found. This is a fairly open ask. Don't just execute code. Tell me and show me something interesting!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d985318-7969-42b4-802e-3cfbe5b55cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53df3047-a64e-41d7-9f1d-bcb9f07395a9",
   "metadata": {},
   "source": [
    "8.Sentiment Analysis\n",
    "Sentiment analysis, also known as opinion mining or emotion AI, is a field of natural language processing (NLP) that focuses on identifying and categorizing opinions or sentiments expressed within text data. The primary goal is to determine the writer's or speaker's attitude towards a particular topic, product, service, or overall context. This attitude can range from positive, negative, to neutral, and may also encompass more nuanced emotions like happiness, anger, sadness, quantitative or emotional.\n",
    "\n",
    "\n",
    "The most basic form of sentiment analysis assigns values to words based on a dictionary of words, from neutral to slightly positive or negative, moderately positive or negative, and extremely positive or negative. Vader is a popular package that analyzes sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3efe5f21-0b6e-4830-baa6-0e534deacf54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting vaderSentiment\n",
      "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 kB\u001b[0m \u001b[31m144.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from vaderSentiment) (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->vaderSentiment) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->vaderSentiment) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->vaderSentiment) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from requests->vaderSentiment) (2023.5.7)\n",
      "Installing collected packages: vaderSentiment\n",
      "Successfully installed vaderSentiment-3.3.2\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#run this\n",
    "!pip install vaderSentiment\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e0ed00-117c-4c03-b17b-ce70f89dedb3",
   "metadata": {},
   "source": [
    "\n",
    "This will compute a compound score by summing the valence scores of each word int he lexicon and then bnormalized between -1 (most negative) and +1 most positive. This is the most useful metric if you want a single unidimensional measure of sentiment for a given sentence. Calling it a \"normalize, weighted composite score\" is accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae180ca0-466c-42ef-9c01-b99f3bb4cd73",
   "metadata": {},
   "source": [
    "let's test opur first sentiment using VADER. VADER is great on social media data which can be messy and contain emojis.\n",
    "We will use the polarity_scores( ) method to obtain the polarity indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a0ae2bf3-715e-4c59-aada-77cb5b56865a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0.0, 'neu': 0.4, 'pos': 0.6, 'compound': 0.6696}\n"
     ]
    }
   ],
   "source": [
    "sentence = \"I love that movie!\"\n",
    "score = analyzer.polarity_scores(sentence)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eab349ee-9595-4bb9-be2f-9b3408c02bb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0.507, 'neu': 0.493, 'pos': 0.0, 'compound': -0.6261}\n"
     ]
    }
   ],
   "source": [
    "sentence2 = \"OMG, this job totally SUX!!\"\n",
    "score = analyzer.polarity_scores(sentence2)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2299e0f2-1739-4892-9007-31e9acfaedd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0.0, 'neu': 0.286, 'pos': 0.714, 'compound': 0.3612}\n"
     ]
    }
   ],
   "source": [
    "sentence3 = '😀' #command+control+space brings up emoji'\n",
    "score = analyzer.polarity_scores(sentence3)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d25327b5-995f-45ba-b813-9f851758c44a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'neg': 0.0, 'neu': 0.286, 'pos': 0.714, 'compound': 0.8885}\n"
     ]
    }
   ],
   "source": [
    "sentence4 = '😀 😀 😀 😀 😀' #command+control+space brings up emoji'\n",
    "score = analyzer.polarity_scores(sentence4)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b3f4b4a-90c9-439f-b1fa-6560a94eb0f8",
   "metadata": {},
   "source": [
    "The global financial crisis (GFC) refers to the period of extreme stress in global financial markets and banking systems between mid 2007 and early 2009. During the GFC, a downturn in the US housing market was a catalyst for a financial crisis that spread from the United States to the rest of the world through linkages in the global financial system. Many banks around the world incurred large losses and relied on government support to avoid bankruptcy. Millions of people lost their jobs as the major advanced economies experienced their deepest recessions since the Great Depression in the 1930s.\n",
    "\n",
    "The Federal Reserve conference calls are a good way to track the sentiment that the Fed had toward the crisis. Did they know how bad it was? Did they think they could fix it? Starting in January of 2008, ending in October of 2008 map the sentiment of the Fed reserve calls.\n",
    "\n",
    "Here are your documents:\n",
    "Jan 2008: https://www.federalreserve.gov/monetarypolicy/files/FOMC20080121confcall.pdf\n",
    "Mar 2008: https://www.federalreserve.gov/monetarypolicy/files/FOMC20080310confcall.pdf\n",
    "July 2008: https://www.federalreserve.gov/monetarypolicy/files/FOMC20080724confcall.pdf\n",
    "Sept 2008: https://www.federalreserve.gov/monetarypolicy/files/FOMC20080929confcall.pdf\n",
    "Oct 2008: https://www.federalreserve.gov/monetarypolicy/files/FOMC20081007confcall.pdf\n",
    "\n",
    "\n",
    "You will need to convert these pdf into text to process. \n",
    "1. Perform a word frequency analysis of each of the calls. Don't forget to remove stopwords.\n",
    "2. Find and present any evidence that the Fed understands that what the US and world is going through is unlike anything that has ever before been experienced. What are the key phrases that convey this idea? How do you find them? Can you use a vector based approach or would you use a custom dictionary?\n",
    "3. Extract any evidence that they thought that the situation could be quickly remedied and would not get as bad as it ended up gettting.\n",
    "4. Parse the key participants and plot their sentiment over time. Who is the most positive? Who is the most negative? Who changes the most?\n",
    "5. Add key makers to your visualization that bring in the key events of 2008.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ea8f4a-1cf9-4ace-bffa-c45f19380b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6f4772-3593-4372-bf0a-ee00a5f2de22",
   "metadata": {},
   "source": [
    "9. Sentiment analysis is not a perfect science, especially when you are using off-the-shelf packages like VADER. Given what you know about the crisis, do you trust Vader's sentiment analysis? WHy or why not?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71eaa8dd-e2d9-4edf-bee7-c4d6e81e4098",
   "metadata": {},
   "source": [
    "#Your answer here:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b12f39-d9c9-42a2-8cab-351209656845",
   "metadata": {},
   "source": [
    "\n",
    "10. Can you reasonably determine whether a low or negative compound score indicates a negative sentiment from the Fed in 2008? Can you do so with data only from 2008?\n",
    "\n",
    "Provide a reasonable comparison from 2008 values by comparing it to some other timeframe from the FED confernce call historic database. https://www.federalreserve.gov/monetarypolicy/fomc_historical_year.htm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfb0c38-720a-4b68-917e-1a3849d5b833",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your code here:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
